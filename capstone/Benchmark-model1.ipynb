{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the dataset\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    fish_files = np.array(data['filenames'])\n",
    "    fish_target = np_utils.to_categorical(np.array(data['target']), 8)\n",
    "    return fish_files,fish_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3777 images in training dataset\n",
      "There are 13153 images in the training set\n"
     ]
    }
   ],
   "source": [
    "#let's load the training-data\n",
    "train_files, train_targets = load_dataset('data/train')\n",
    "\n",
    "#let's load the teting-data\n",
    "test_files, _ = load_dataset('data/test')\n",
    "\n",
    "#print the number of samples in test and trainin sets\n",
    "print (\"There are %d images in training dataset\"%len(train_files))\n",
    "print (\"There are %d images in the training set\"%len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "#converting image to tensor\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image\n",
    "    img = image.load_img(img_path, target_size=(224,224))\n",
    "    #convering the image to 3-D tensor with shape (224,224,3)\n",
    "    x = image.img_to_array(img)\n",
    "    #convert 3D tensor to 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13153/13153 [02:06<00:00, 103.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "\n",
    "#preprocessing the data\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3777/3777 [01:38<00:00, 41.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3777, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#shape of the tensor\n",
    "print(np.shape(train_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's have the Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 110, 110, 32)      2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 2,552\n",
      "Trainable params: 2,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's have a Benchmark model\n",
    "\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "benchmark = Sequential()\n",
    "\n",
    "# model Convolution layer\n",
    "benchmark.add(Conv2D(filters=16,kernel_size=2,strides=1,activation='relu',input_shape=(224,224,3)))\n",
    "# Max Pooling layer to reduce the dimensionality\n",
    "benchmark.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "#Dropout layer, for turning off each node with the probability of 0.3\n",
    "benchmark.add(Dropout(0.3))\n",
    "benchmark.add(Conv2D(filters=32, kernel_size=2,strides=1,activation='relu'))\n",
    "benchmark.add(Dropout(0.3))\n",
    "benchmark.add(GlobalAveragePooling2D())\n",
    "#A fully connected dense layer with 8 nodes (no of classes of fish)\n",
    "benchmark.add(Dense(8,activation='softmax'))\n",
    "benchmark.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3021 samples, validate on 756 samples\n",
      "Epoch 1/10\n",
      "3021/3021 [==============================] - ETA: 16:33 - loss: 1.9862 - acc: 0.05 - ETA: 5:30 - loss: 1.9573 - acc: 0.1833 - ETA: 3:17 - loss: 1.9341 - acc: 0.300 - ETA: 2:20 - loss: 1.9107 - acc: 0.350 - ETA: 1:48 - loss: 1.9088 - acc: 0.361 - ETA: 1:28 - loss: 1.8724 - acc: 0.400 - ETA: 1:14 - loss: 1.8683 - acc: 0.403 - ETA: 1:04 - loss: 1.8572 - acc: 0.406 - ETA: 56s - loss: 1.8547 - acc: 0.400 - ETA: 50s - loss: 1.8548 - acc: 0.39 - ETA: 45s - loss: 1.8286 - acc: 0.40 - ETA: 41s - loss: 1.8137 - acc: 0.41 - ETA: 37s - loss: 1.7956 - acc: 0.41 - ETA: 34s - loss: 1.8016 - acc: 0.41 - ETA: 31s - loss: 1.7868 - acc: 0.42 - ETA: 29s - loss: 1.7766 - acc: 0.42 - ETA: 27s - loss: 1.7811 - acc: 0.41 - ETA: 25s - loss: 1.7723 - acc: 0.42 - ETA: 24s - loss: 1.7620 - acc: 0.42 - ETA: 22s - loss: 1.7607 - acc: 0.42 - ETA: 21s - loss: 1.7520 - acc: 0.42 - ETA: 20s - loss: 1.7526 - acc: 0.41 - ETA: 19s - loss: 1.7438 - acc: 0.42 - ETA: 18s - loss: 1.7485 - acc: 0.41 - ETA: 17s - loss: 1.7473 - acc: 0.41 - ETA: 16s - loss: 1.7420 - acc: 0.41 - ETA: 15s - loss: 1.7321 - acc: 0.42 - ETA: 14s - loss: 1.7285 - acc: 0.42 - ETA: 13s - loss: 1.7255 - acc: 0.42 - ETA: 13s - loss: 1.7215 - acc: 0.42 - ETA: 12s - loss: 1.7146 - acc: 0.42 - ETA: 12s - loss: 1.7136 - acc: 0.42 - ETA: 11s - loss: 1.7097 - acc: 0.42 - ETA: 11s - loss: 1.7070 - acc: 0.42 - ETA: 10s - loss: 1.7140 - acc: 0.42 - ETA: 10s - loss: 1.7086 - acc: 0.43 - ETA: 9s - loss: 1.7073 - acc: 0.4301 - ETA: 9s - loss: 1.7042 - acc: 0.430 - ETA: 8s - loss: 1.7061 - acc: 0.429 - ETA: 8s - loss: 1.7023 - acc: 0.431 - ETA: 8s - loss: 1.7013 - acc: 0.432 - ETA: 7s - loss: 1.6989 - acc: 0.431 - ETA: 7s - loss: 1.6994 - acc: 0.431 - ETA: 6s - loss: 1.6993 - acc: 0.429 - ETA: 6s - loss: 1.6965 - acc: 0.431 - ETA: 6s - loss: 1.6926 - acc: 0.433 - ETA: 6s - loss: 1.6878 - acc: 0.434 - ETA: 5s - loss: 1.6906 - acc: 0.434 - ETA: 5s - loss: 1.6895 - acc: 0.436 - ETA: 5s - loss: 1.6840 - acc: 0.438 - ETA: 4s - loss: 1.6807 - acc: 0.440 - ETA: 4s - loss: 1.6769 - acc: 0.441 - ETA: 4s - loss: 1.6730 - acc: 0.443 - ETA: 4s - loss: 1.6706 - acc: 0.445 - ETA: 3s - loss: 1.6693 - acc: 0.445 - ETA: 3s - loss: 1.6658 - acc: 0.447 - ETA: 3s - loss: 1.6656 - acc: 0.448 - ETA: 3s - loss: 1.6647 - acc: 0.447 - ETA: 3s - loss: 1.6609 - acc: 0.449 - ETA: 2s - loss: 1.6620 - acc: 0.448 - ETA: 2s - loss: 1.6596 - acc: 0.449 - ETA: 2s - loss: 1.6570 - acc: 0.450 - ETA: 2s - loss: 1.6575 - acc: 0.449 - ETA: 2s - loss: 1.6571 - acc: 0.448 - ETA: 1s - loss: 1.6570 - acc: 0.447 - ETA: 1s - loss: 1.6562 - acc: 0.448 - ETA: 1s - loss: 1.6547 - acc: 0.448 - ETA: 1s - loss: 1.6545 - acc: 0.446 - ETA: 1s - loss: 1.6517 - acc: 0.446 - ETA: 0s - loss: 1.6505 - acc: 0.447 - ETA: 0s - loss: 1.6484 - acc: 0.448 - ETA: 0s - loss: 1.6492 - acc: 0.448 - ETA: 0s - loss: 1.6507 - acc: 0.447 - ETA: 0s - loss: 1.6460 - acc: 0.448 - ETA: 0s - loss: 1.6450 - acc: 0.449 - ETA: 0s - loss: 1.6443 - acc: 0.449 - 12s 4ms/step - loss: 1.6445 - acc: 0.4489 - val_loss: 1.6103 - val_acc: 0.4643\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61031, saving model to saved_models/weights.best.benchmark.hdf5\n",
      "Epoch 2/10\n",
      "3021/3021 [==============================] - ETA: 4s - loss: 1.9792 - acc: 0.300 - ETA: 4s - loss: 1.5874 - acc: 0.466 - ETA: 4s - loss: 1.5703 - acc: 0.480 - ETA: 4s - loss: 1.5861 - acc: 0.464 - ETA: 4s - loss: 1.5823 - acc: 0.461 - ETA: 4s - loss: 1.6060 - acc: 0.445 - ETA: 4s - loss: 1.6436 - acc: 0.423 - ETA: 4s - loss: 1.6550 - acc: 0.426 - ETA: 4s - loss: 1.6505 - acc: 0.429 - ETA: 4s - loss: 1.6487 - acc: 0.434 - ETA: 4s - loss: 1.6629 - acc: 0.423 - ETA: 4s - loss: 1.6428 - acc: 0.432 - ETA: 4s - loss: 1.6323 - acc: 0.438 - ETA: 4s - loss: 1.6201 - acc: 0.444 - ETA: 3s - loss: 1.6212 - acc: 0.443 - ETA: 3s - loss: 1.6347 - acc: 0.441 - ETA: 3s - loss: 1.6270 - acc: 0.443 - ETA: 3s - loss: 1.6147 - acc: 0.447 - ETA: 3s - loss: 1.6260 - acc: 0.436 - ETA: 3s - loss: 1.6288 - acc: 0.432 - ETA: 3s - loss: 1.6293 - acc: 0.434 - ETA: 3s - loss: 1.6405 - acc: 0.430 - ETA: 3s - loss: 1.6358 - acc: 0.436 - ETA: 3s - loss: 1.6279 - acc: 0.441 - ETA: 3s - loss: 1.6213 - acc: 0.445 - ETA: 3s - loss: 1.6224 - acc: 0.444 - ETA: 3s - loss: 1.6252 - acc: 0.444 - ETA: 3s - loss: 1.6190 - acc: 0.444 - ETA: 3s - loss: 1.6069 - acc: 0.449 - ETA: 3s - loss: 1.6129 - acc: 0.447 - ETA: 3s - loss: 1.6189 - acc: 0.444 - ETA: 2s - loss: 1.6198 - acc: 0.442 - ETA: 2s - loss: 1.6154 - acc: 0.446 - ETA: 2s - loss: 1.6118 - acc: 0.447 - ETA: 2s - loss: 1.6084 - acc: 0.449 - ETA: 2s - loss: 1.6015 - acc: 0.452 - ETA: 2s - loss: 1.5999 - acc: 0.452 - ETA: 2s - loss: 1.6010 - acc: 0.452 - ETA: 2s - loss: 1.6057 - acc: 0.451 - ETA: 2s - loss: 1.6025 - acc: 0.451 - ETA: 2s - loss: 1.6074 - acc: 0.448 - ETA: 2s - loss: 1.6050 - acc: 0.450 - ETA: 2s - loss: 1.6035 - acc: 0.449 - ETA: 2s - loss: 1.6036 - acc: 0.450 - ETA: 2s - loss: 1.5974 - acc: 0.452 - ETA: 1s - loss: 1.6020 - acc: 0.450 - ETA: 1s - loss: 1.6018 - acc: 0.452 - ETA: 1s - loss: 1.6053 - acc: 0.450 - ETA: 1s - loss: 1.6052 - acc: 0.450 - ETA: 1s - loss: 1.6040 - acc: 0.450 - ETA: 1s - loss: 1.6035 - acc: 0.450 - ETA: 1s - loss: 1.6083 - acc: 0.449 - ETA: 1s - loss: 1.6084 - acc: 0.449 - ETA: 1s - loss: 1.6102 - acc: 0.448 - ETA: 1s - loss: 1.6089 - acc: 0.450 - ETA: 1s - loss: 1.6078 - acc: 0.450 - ETA: 1s - loss: 1.6062 - acc: 0.451 - ETA: 1s - loss: 1.6068 - acc: 0.451 - ETA: 1s - loss: 1.6052 - acc: 0.451 - ETA: 1s - loss: 1.6033 - acc: 0.451 - ETA: 0s - loss: 1.6040 - acc: 0.451 - ETA: 0s - loss: 1.6068 - acc: 0.450 - ETA: 0s - loss: 1.6042 - acc: 0.452 - ETA: 0s - loss: 1.6078 - acc: 0.450 - ETA: 0s - loss: 1.6056 - acc: 0.450 - ETA: 0s - loss: 1.6073 - acc: 0.449 - ETA: 0s - loss: 1.6040 - acc: 0.450 - ETA: 0s - loss: 1.6006 - acc: 0.451 - ETA: 0s - loss: 1.6064 - acc: 0.449 - ETA: 0s - loss: 1.6043 - acc: 0.450 - ETA: 0s - loss: 1.6039 - acc: 0.449 - ETA: 0s - loss: 1.6059 - acc: 0.448 - ETA: 0s - loss: 1.6032 - acc: 0.449 - ETA: 0s - loss: 1.6017 - acc: 0.450 - ETA: 0s - loss: 1.5994 - acc: 0.451 - ETA: 0s - loss: 1.5978 - acc: 0.452 - 5s 2ms/step - loss: 1.5976 - acc: 0.4528 - val_loss: 1.6374 - val_acc: 0.4643\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.61031\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3021/3021 [==============================] - ETA: 4s - loss: 1.5784 - acc: 0.450 - ETA: 4s - loss: 1.5690 - acc: 0.450 - ETA: 4s - loss: 1.5960 - acc: 0.400 - ETA: 4s - loss: 1.5987 - acc: 0.421 - ETA: 4s - loss: 1.6391 - acc: 0.400 - ETA: 4s - loss: 1.6314 - acc: 0.422 - ETA: 4s - loss: 1.6402 - acc: 0.426 - ETA: 4s - loss: 1.6452 - acc: 0.426 - ETA: 4s - loss: 1.6683 - acc: 0.411 - ETA: 4s - loss: 1.6367 - acc: 0.439 - ETA: 4s - loss: 1.6214 - acc: 0.447 - ETA: 3s - loss: 1.6319 - acc: 0.437 - ETA: 3s - loss: 1.6206 - acc: 0.438 - ETA: 3s - loss: 1.6144 - acc: 0.446 - ETA: 3s - loss: 1.6165 - acc: 0.446 - ETA: 3s - loss: 1.6152 - acc: 0.448 - ETA: 3s - loss: 1.6114 - acc: 0.453 - ETA: 3s - loss: 1.5921 - acc: 0.461 - ETA: 3s - loss: 1.5968 - acc: 0.459 - ETA: 3s - loss: 1.5984 - acc: 0.456 - ETA: 3s - loss: 1.6016 - acc: 0.456 - ETA: 3s - loss: 1.6050 - acc: 0.453 - ETA: 3s - loss: 1.6086 - acc: 0.448 - ETA: 3s - loss: 1.6065 - acc: 0.451 - ETA: 3s - loss: 1.6013 - acc: 0.452 - ETA: 3s - loss: 1.5971 - acc: 0.453 - ETA: 3s - loss: 1.5969 - acc: 0.456 - ETA: 3s - loss: 1.6016 - acc: 0.456 - ETA: 3s - loss: 1.6077 - acc: 0.450 - ETA: 2s - loss: 1.5994 - acc: 0.455 - ETA: 2s - loss: 1.5936 - acc: 0.456 - ETA: 2s - loss: 1.5893 - acc: 0.458 - ETA: 2s - loss: 1.5894 - acc: 0.460 - ETA: 2s - loss: 1.5926 - acc: 0.456 - ETA: 2s - loss: 1.5916 - acc: 0.455 - ETA: 2s - loss: 1.5888 - acc: 0.457 - ETA: 2s - loss: 1.5819 - acc: 0.461 - ETA: 2s - loss: 1.5790 - acc: 0.460 - ETA: 2s - loss: 1.5791 - acc: 0.459 - ETA: 2s - loss: 1.5826 - acc: 0.457 - ETA: 2s - loss: 1.5808 - acc: 0.457 - ETA: 2s - loss: 1.5768 - acc: 0.459 - ETA: 2s - loss: 1.5780 - acc: 0.458 - ETA: 2s - loss: 1.5780 - acc: 0.459 - ETA: 2s - loss: 1.5809 - acc: 0.457 - ETA: 1s - loss: 1.5800 - acc: 0.459 - ETA: 1s - loss: 1.5815 - acc: 0.459 - ETA: 1s - loss: 1.5829 - acc: 0.459 - ETA: 1s - loss: 1.5782 - acc: 0.460 - ETA: 1s - loss: 1.5808 - acc: 0.460 - ETA: 1s - loss: 1.5819 - acc: 0.460 - ETA: 1s - loss: 1.5806 - acc: 0.460 - ETA: 1s - loss: 1.5785 - acc: 0.461 - ETA: 1s - loss: 1.5718 - acc: 0.464 - ETA: 1s - loss: 1.5719 - acc: 0.462 - ETA: 1s - loss: 1.5687 - acc: 0.463 - ETA: 1s - loss: 1.5714 - acc: 0.463 - ETA: 1s - loss: 1.5779 - acc: 0.460 - ETA: 1s - loss: 1.5795 - acc: 0.460 - ETA: 1s - loss: 1.5808 - acc: 0.460 - ETA: 1s - loss: 1.5781 - acc: 0.460 - ETA: 0s - loss: 1.5806 - acc: 0.459 - ETA: 0s - loss: 1.5765 - acc: 0.462 - ETA: 0s - loss: 1.5788 - acc: 0.462 - ETA: 0s - loss: 1.5814 - acc: 0.461 - ETA: 0s - loss: 1.5834 - acc: 0.460 - ETA: 0s - loss: 1.5855 - acc: 0.458 - ETA: 0s - loss: 1.5857 - acc: 0.457 - ETA: 0s - loss: 1.5874 - acc: 0.457 - ETA: 0s - loss: 1.5838 - acc: 0.458 - ETA: 0s - loss: 1.5842 - acc: 0.457 - ETA: 0s - loss: 1.5849 - acc: 0.457 - ETA: 0s - loss: 1.5890 - acc: 0.455 - ETA: 0s - loss: 1.5875 - acc: 0.456 - ETA: 0s - loss: 1.5877 - acc: 0.457 - ETA: 0s - loss: 1.5859 - acc: 0.458 - ETA: 0s - loss: 1.5870 - acc: 0.457 - 5s 2ms/step - loss: 1.5867 - acc: 0.4578 - val_loss: 1.5903 - val_acc: 0.4643\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.61031 to 1.59026, saving model to saved_models/weights.best.benchmark.hdf5\n",
      "Epoch 4/10\n",
      "3021/3021 [==============================] - ETA: 4s - loss: 1.6188 - acc: 0.450 - ETA: 5s - loss: 1.7235 - acc: 0.433 - ETA: 5s - loss: 1.6948 - acc: 0.430 - ETA: 4s - loss: 1.6778 - acc: 0.442 - ETA: 4s - loss: 1.6304 - acc: 0.461 - ETA: 4s - loss: 1.6481 - acc: 0.454 - ETA: 4s - loss: 1.6508 - acc: 0.450 - ETA: 4s - loss: 1.6502 - acc: 0.450 - ETA: 4s - loss: 1.6425 - acc: 0.447 - ETA: 4s - loss: 1.6426 - acc: 0.444 - ETA: 4s - loss: 1.6383 - acc: 0.445 - ETA: 4s - loss: 1.6260 - acc: 0.452 - ETA: 4s - loss: 1.6352 - acc: 0.444 - ETA: 4s - loss: 1.6259 - acc: 0.448 - ETA: 3s - loss: 1.6238 - acc: 0.455 - ETA: 3s - loss: 1.6133 - acc: 0.464 - ETA: 3s - loss: 1.6171 - acc: 0.465 - ETA: 3s - loss: 1.6105 - acc: 0.465 - ETA: 3s - loss: 1.6055 - acc: 0.463 - ETA: 3s - loss: 1.5964 - acc: 0.464 - ETA: 3s - loss: 1.6030 - acc: 0.461 - ETA: 3s - loss: 1.6027 - acc: 0.460 - ETA: 3s - loss: 1.6189 - acc: 0.457 - ETA: 3s - loss: 1.6282 - acc: 0.453 - ETA: 3s - loss: 1.6243 - acc: 0.456 - ETA: 3s - loss: 1.6164 - acc: 0.459 - ETA: 3s - loss: 1.6126 - acc: 0.460 - ETA: 3s - loss: 1.6011 - acc: 0.466 - ETA: 2s - loss: 1.5954 - acc: 0.468 - ETA: 2s - loss: 1.5850 - acc: 0.469 - ETA: 2s - loss: 1.5836 - acc: 0.469 - ETA: 2s - loss: 1.5844 - acc: 0.465 - ETA: 2s - loss: 1.5850 - acc: 0.466 - ETA: 2s - loss: 1.5845 - acc: 0.467 - ETA: 2s - loss: 1.5755 - acc: 0.471 - ETA: 2s - loss: 1.5794 - acc: 0.469 - ETA: 2s - loss: 1.5786 - acc: 0.469 - ETA: 2s - loss: 1.5827 - acc: 0.468 - ETA: 2s - loss: 1.5800 - acc: 0.468 - ETA: 2s - loss: 1.5807 - acc: 0.468 - ETA: 2s - loss: 1.5877 - acc: 0.463 - ETA: 2s - loss: 1.5903 - acc: 0.460 - ETA: 2s - loss: 1.5871 - acc: 0.460 - ETA: 2s - loss: 1.5852 - acc: 0.462 - ETA: 2s - loss: 1.5827 - acc: 0.463 - ETA: 1s - loss: 1.5779 - acc: 0.464 - ETA: 1s - loss: 1.5719 - acc: 0.467 - ETA: 1s - loss: 1.5713 - acc: 0.467 - ETA: 1s - loss: 1.5678 - acc: 0.467 - ETA: 1s - loss: 1.5680 - acc: 0.467 - ETA: 1s - loss: 1.5682 - acc: 0.467 - ETA: 1s - loss: 1.5697 - acc: 0.467 - ETA: 1s - loss: 1.5657 - acc: 0.468 - ETA: 1s - loss: 1.5687 - acc: 0.467 - ETA: 1s - loss: 1.5688 - acc: 0.467 - ETA: 1s - loss: 1.5700 - acc: 0.465 - ETA: 1s - loss: 1.5707 - acc: 0.465 - ETA: 1s - loss: 1.5708 - acc: 0.465 - ETA: 1s - loss: 1.5670 - acc: 0.466 - ETA: 1s - loss: 1.5661 - acc: 0.467 - ETA: 0s - loss: 1.5684 - acc: 0.466 - ETA: 0s - loss: 1.5683 - acc: 0.467 - ETA: 0s - loss: 1.5722 - acc: 0.466 - ETA: 0s - loss: 1.5714 - acc: 0.466 - ETA: 0s - loss: 1.5742 - acc: 0.466 - ETA: 0s - loss: 1.5695 - acc: 0.468 - ETA: 0s - loss: 1.5692 - acc: 0.468 - ETA: 0s - loss: 1.5689 - acc: 0.467 - ETA: 0s - loss: 1.5666 - acc: 0.468 - ETA: 0s - loss: 1.5685 - acc: 0.468 - ETA: 0s - loss: 1.5710 - acc: 0.466 - ETA: 0s - loss: 1.5706 - acc: 0.465 - ETA: 0s - loss: 1.5697 - acc: 0.464 - ETA: 0s - loss: 1.5707 - acc: 0.464 - ETA: 0s - loss: 1.5719 - acc: 0.464 - ETA: 0s - loss: 1.5726 - acc: 0.463 - 5s 2ms/step - loss: 1.5712 - acc: 0.4641 - val_loss: 1.5727 - val_acc: 0.4828\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.59026 to 1.57265, saving model to saved_models/weights.best.benchmark.hdf5\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3021/3021 [==============================] - ETA: 5s - loss: 1.6758 - acc: 0.500 - ETA: 4s - loss: 1.4223 - acc: 0.583 - ETA: 4s - loss: 1.5220 - acc: 0.510 - ETA: 4s - loss: 1.5340 - acc: 0.471 - ETA: 4s - loss: 1.5321 - acc: 0.477 - ETA: 4s - loss: 1.5826 - acc: 0.463 - ETA: 4s - loss: 1.5686 - acc: 0.473 - ETA: 4s - loss: 1.5489 - acc: 0.480 - ETA: 4s - loss: 1.5405 - acc: 0.488 - ETA: 4s - loss: 1.5418 - acc: 0.489 - ETA: 4s - loss: 1.5387 - acc: 0.495 - ETA: 4s - loss: 1.5514 - acc: 0.484 - ETA: 4s - loss: 1.5385 - acc: 0.492 - ETA: 4s - loss: 1.5432 - acc: 0.492 - ETA: 3s - loss: 1.5365 - acc: 0.498 - ETA: 3s - loss: 1.5438 - acc: 0.493 - ETA: 3s - loss: 1.5597 - acc: 0.481 - ETA: 3s - loss: 1.5543 - acc: 0.478 - ETA: 3s - loss: 1.5494 - acc: 0.483 - ETA: 3s - loss: 1.5549 - acc: 0.480 - ETA: 3s - loss: 1.5545 - acc: 0.478 - ETA: 3s - loss: 1.5544 - acc: 0.477 - ETA: 3s - loss: 1.5643 - acc: 0.472 - ETA: 3s - loss: 1.5595 - acc: 0.474 - ETA: 3s - loss: 1.5540 - acc: 0.475 - ETA: 3s - loss: 1.5541 - acc: 0.475 - ETA: 3s - loss: 1.5590 - acc: 0.474 - ETA: 3s - loss: 1.5555 - acc: 0.476 - ETA: 3s - loss: 1.5517 - acc: 0.477 - ETA: 2s - loss: 1.5580 - acc: 0.474 - ETA: 2s - loss: 1.5554 - acc: 0.474 - ETA: 2s - loss: 1.5605 - acc: 0.474 - ETA: 2s - loss: 1.5577 - acc: 0.474 - ETA: 2s - loss: 1.5630 - acc: 0.473 - ETA: 2s - loss: 1.5628 - acc: 0.473 - ETA: 2s - loss: 1.5590 - acc: 0.475 - ETA: 2s - loss: 1.5558 - acc: 0.476 - ETA: 2s - loss: 1.5604 - acc: 0.476 - ETA: 2s - loss: 1.5606 - acc: 0.477 - ETA: 2s - loss: 1.5617 - acc: 0.476 - ETA: 2s - loss: 1.5674 - acc: 0.473 - ETA: 2s - loss: 1.5710 - acc: 0.472 - ETA: 2s - loss: 1.5741 - acc: 0.470 - ETA: 2s - loss: 1.5736 - acc: 0.467 - ETA: 2s - loss: 1.5730 - acc: 0.469 - ETA: 1s - loss: 1.5709 - acc: 0.470 - ETA: 1s - loss: 1.5701 - acc: 0.469 - ETA: 1s - loss: 1.5657 - acc: 0.470 - ETA: 1s - loss: 1.5693 - acc: 0.468 - ETA: 1s - loss: 1.5644 - acc: 0.471 - ETA: 1s - loss: 1.5627 - acc: 0.472 - ETA: 1s - loss: 1.5644 - acc: 0.470 - ETA: 1s - loss: 1.5682 - acc: 0.468 - ETA: 1s - loss: 1.5668 - acc: 0.468 - ETA: 1s - loss: 1.5690 - acc: 0.467 - ETA: 1s - loss: 1.5676 - acc: 0.467 - ETA: 1s - loss: 1.5686 - acc: 0.467 - ETA: 1s - loss: 1.5741 - acc: 0.464 - ETA: 1s - loss: 1.5764 - acc: 0.463 - ETA: 1s - loss: 1.5756 - acc: 0.464 - ETA: 1s - loss: 1.5755 - acc: 0.464 - ETA: 0s - loss: 1.5731 - acc: 0.465 - ETA: 0s - loss: 1.5716 - acc: 0.465 - ETA: 0s - loss: 1.5676 - acc: 0.466 - ETA: 0s - loss: 1.5667 - acc: 0.466 - ETA: 0s - loss: 1.5661 - acc: 0.465 - ETA: 0s - loss: 1.5629 - acc: 0.466 - ETA: 0s - loss: 1.5675 - acc: 0.463 - ETA: 0s - loss: 1.5666 - acc: 0.464 - ETA: 0s - loss: 1.5660 - acc: 0.463 - ETA: 0s - loss: 1.5642 - acc: 0.463 - ETA: 0s - loss: 1.5651 - acc: 0.463 - ETA: 0s - loss: 1.5660 - acc: 0.463 - ETA: 0s - loss: 1.5643 - acc: 0.463 - ETA: 0s - loss: 1.5613 - acc: 0.464 - ETA: 0s - loss: 1.5627 - acc: 0.464 - 5s 2ms/step - loss: 1.5609 - acc: 0.4651 - val_loss: 1.5749 - val_acc: 0.4788\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.57265\n",
      "Epoch 6/10\n",
      "3021/3021 [==============================] - ETA: 4s - loss: 1.8365 - acc: 0.350 - ETA: 4s - loss: 1.6688 - acc: 0.433 - ETA: 4s - loss: 1.6294 - acc: 0.410 - ETA: 4s - loss: 1.6688 - acc: 0.392 - ETA: 4s - loss: 1.6309 - acc: 0.422 - ETA: 4s - loss: 1.6191 - acc: 0.440 - ETA: 4s - loss: 1.6150 - acc: 0.438 - ETA: 4s - loss: 1.5910 - acc: 0.453 - ETA: 4s - loss: 1.5880 - acc: 0.461 - ETA: 4s - loss: 1.5754 - acc: 0.468 - ETA: 4s - loss: 1.5842 - acc: 0.469 - ETA: 4s - loss: 1.5568 - acc: 0.476 - ETA: 3s - loss: 1.5389 - acc: 0.480 - ETA: 3s - loss: 1.5465 - acc: 0.472 - ETA: 3s - loss: 1.5606 - acc: 0.465 - ETA: 3s - loss: 1.5601 - acc: 0.464 - ETA: 3s - loss: 1.5705 - acc: 0.454 - ETA: 3s - loss: 1.5723 - acc: 0.451 - ETA: 3s - loss: 1.5603 - acc: 0.458 - ETA: 3s - loss: 1.5588 - acc: 0.459 - ETA: 3s - loss: 1.5717 - acc: 0.452 - ETA: 3s - loss: 1.5853 - acc: 0.448 - ETA: 3s - loss: 1.5839 - acc: 0.453 - ETA: 3s - loss: 1.5894 - acc: 0.452 - ETA: 3s - loss: 1.5870 - acc: 0.453 - ETA: 3s - loss: 1.5838 - acc: 0.453 - ETA: 3s - loss: 1.5788 - acc: 0.454 - ETA: 3s - loss: 1.5810 - acc: 0.453 - ETA: 2s - loss: 1.5813 - acc: 0.450 - ETA: 2s - loss: 1.5842 - acc: 0.450 - ETA: 2s - loss: 1.5776 - acc: 0.453 - ETA: 2s - loss: 1.5773 - acc: 0.451 - ETA: 2s - loss: 1.5832 - acc: 0.449 - ETA: 2s - loss: 1.5787 - acc: 0.450 - ETA: 2s - loss: 1.5796 - acc: 0.449 - ETA: 2s - loss: 1.5816 - acc: 0.449 - ETA: 2s - loss: 1.5790 - acc: 0.452 - ETA: 2s - loss: 1.5716 - acc: 0.456 - ETA: 2s - loss: 1.5658 - acc: 0.457 - ETA: 2s - loss: 1.5644 - acc: 0.460 - ETA: 2s - loss: 1.5701 - acc: 0.458 - ETA: 2s - loss: 1.5650 - acc: 0.461 - ETA: 2s - loss: 1.5617 - acc: 0.462 - ETA: 2s - loss: 1.5631 - acc: 0.462 - ETA: 2s - loss: 1.5641 - acc: 0.462 - ETA: 1s - loss: 1.5672 - acc: 0.461 - ETA: 1s - loss: 1.5685 - acc: 0.460 - ETA: 1s - loss: 1.5632 - acc: 0.462 - ETA: 1s - loss: 1.5620 - acc: 0.463 - ETA: 1s - loss: 1.5584 - acc: 0.465 - ETA: 1s - loss: 1.5558 - acc: 0.466 - ETA: 1s - loss: 1.5546 - acc: 0.466 - ETA: 1s - loss: 1.5523 - acc: 0.465 - ETA: 1s - loss: 1.5493 - acc: 0.466 - ETA: 1s - loss: 1.5525 - acc: 0.464 - ETA: 1s - loss: 1.5486 - acc: 0.466 - ETA: 1s - loss: 1.5474 - acc: 0.466 - ETA: 1s - loss: 1.5505 - acc: 0.463 - ETA: 1s - loss: 1.5506 - acc: 0.462 - ETA: 1s - loss: 1.5502 - acc: 0.465 - ETA: 1s - loss: 1.5522 - acc: 0.463 - ETA: 0s - loss: 1.5522 - acc: 0.463 - ETA: 0s - loss: 1.5516 - acc: 0.463 - ETA: 0s - loss: 1.5488 - acc: 0.464 - ETA: 0s - loss: 1.5485 - acc: 0.464 - ETA: 0s - loss: 1.5474 - acc: 0.464 - ETA: 0s - loss: 1.5478 - acc: 0.464 - ETA: 0s - loss: 1.5496 - acc: 0.464 - ETA: 0s - loss: 1.5467 - acc: 0.464 - ETA: 0s - loss: 1.5486 - acc: 0.463 - ETA: 0s - loss: 1.5487 - acc: 0.463 - ETA: 0s - loss: 1.5480 - acc: 0.462 - ETA: 0s - loss: 1.5472 - acc: 0.462 - ETA: 0s - loss: 1.5492 - acc: 0.462 - ETA: 0s - loss: 1.5506 - acc: 0.461 - ETA: 0s - loss: 1.5515 - acc: 0.461 - 5s 2ms/step - loss: 1.5504 - acc: 0.4614 - val_loss: 1.5491 - val_acc: 0.4696\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.57265 to 1.54906, saving model to saved_models/weights.best.benchmark.hdf5\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3021/3021 [==============================] - ETA: 4s - loss: 1.5152 - acc: 0.450 - ETA: 4s - loss: 1.5455 - acc: 0.483 - ETA: 4s - loss: 1.6316 - acc: 0.460 - ETA: 4s - loss: 1.6182 - acc: 0.471 - ETA: 4s - loss: 1.5828 - acc: 0.483 - ETA: 4s - loss: 1.5936 - acc: 0.463 - ETA: 4s - loss: 1.6085 - acc: 0.450 - ETA: 4s - loss: 1.6052 - acc: 0.436 - ETA: 4s - loss: 1.6145 - acc: 0.432 - ETA: 4s - loss: 1.5992 - acc: 0.426 - ETA: 4s - loss: 1.5911 - acc: 0.427 - ETA: 4s - loss: 1.5775 - acc: 0.438 - ETA: 4s - loss: 1.5714 - acc: 0.445 - ETA: 4s - loss: 1.5664 - acc: 0.450 - ETA: 3s - loss: 1.5537 - acc: 0.448 - ETA: 3s - loss: 1.5353 - acc: 0.460 - ETA: 3s - loss: 1.5431 - acc: 0.456 - ETA: 3s - loss: 1.5357 - acc: 0.464 - ETA: 3s - loss: 1.5439 - acc: 0.462 - ETA: 3s - loss: 1.5428 - acc: 0.459 - ETA: 3s - loss: 1.5467 - acc: 0.460 - ETA: 3s - loss: 1.5393 - acc: 0.463 - ETA: 3s - loss: 1.5452 - acc: 0.460 - ETA: 3s - loss: 1.5405 - acc: 0.463 - ETA: 3s - loss: 1.5399 - acc: 0.464 - ETA: 3s - loss: 1.5462 - acc: 0.463 - ETA: 3s - loss: 1.5427 - acc: 0.462 - ETA: 3s - loss: 1.5366 - acc: 0.463 - ETA: 3s - loss: 1.5336 - acc: 0.467 - ETA: 2s - loss: 1.5340 - acc: 0.466 - ETA: 2s - loss: 1.5352 - acc: 0.468 - ETA: 2s - loss: 1.5301 - acc: 0.473 - ETA: 2s - loss: 1.5352 - acc: 0.472 - ETA: 2s - loss: 1.5385 - acc: 0.473 - ETA: 2s - loss: 1.5419 - acc: 0.472 - ETA: 2s - loss: 1.5419 - acc: 0.472 - ETA: 2s - loss: 1.5468 - acc: 0.469 - ETA: 2s - loss: 1.5465 - acc: 0.466 - ETA: 2s - loss: 1.5474 - acc: 0.463 - ETA: 2s - loss: 1.5502 - acc: 0.461 - ETA: 2s - loss: 1.5488 - acc: 0.461 - ETA: 2s - loss: 1.5503 - acc: 0.461 - ETA: 2s - loss: 1.5470 - acc: 0.462 - ETA: 2s - loss: 1.5464 - acc: 0.464 - ETA: 2s - loss: 1.5440 - acc: 0.463 - ETA: 1s - loss: 1.5467 - acc: 0.463 - ETA: 1s - loss: 1.5500 - acc: 0.463 - ETA: 1s - loss: 1.5501 - acc: 0.463 - ETA: 1s - loss: 1.5503 - acc: 0.463 - ETA: 1s - loss: 1.5455 - acc: 0.466 - ETA: 1s - loss: 1.5460 - acc: 0.466 - ETA: 1s - loss: 1.5465 - acc: 0.467 - ETA: 1s - loss: 1.5478 - acc: 0.467 - ETA: 1s - loss: 1.5499 - acc: 0.465 - ETA: 1s - loss: 1.5508 - acc: 0.464 - ETA: 1s - loss: 1.5470 - acc: 0.465 - ETA: 1s - loss: 1.5479 - acc: 0.464 - ETA: 1s - loss: 1.5412 - acc: 0.467 - ETA: 1s - loss: 1.5427 - acc: 0.466 - ETA: 1s - loss: 1.5404 - acc: 0.466 - ETA: 0s - loss: 1.5380 - acc: 0.467 - ETA: 0s - loss: 1.5380 - acc: 0.467 - ETA: 0s - loss: 1.5382 - acc: 0.467 - ETA: 0s - loss: 1.5399 - acc: 0.465 - ETA: 0s - loss: 1.5404 - acc: 0.466 - ETA: 0s - loss: 1.5420 - acc: 0.465 - ETA: 0s - loss: 1.5404 - acc: 0.464 - ETA: 0s - loss: 1.5360 - acc: 0.466 - ETA: 0s - loss: 1.5350 - acc: 0.467 - ETA: 0s - loss: 1.5342 - acc: 0.467 - ETA: 0s - loss: 1.5353 - acc: 0.468 - ETA: 0s - loss: 1.5344 - acc: 0.468 - ETA: 0s - loss: 1.5367 - acc: 0.467 - ETA: 0s - loss: 1.5384 - acc: 0.466 - ETA: 0s - loss: 1.5391 - acc: 0.465 - ETA: 0s - loss: 1.5376 - acc: 0.464 - ETA: 0s - loss: 1.5370 - acc: 0.464 - 5s 2ms/step - loss: 1.5374 - acc: 0.4644 - val_loss: 1.5525 - val_acc: 0.4762\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.54906\n",
      "Epoch 8/10\n",
      "3021/3021 [==============================] - ETA: 4s - loss: 1.7434 - acc: 0.400 - ETA: 4s - loss: 1.6131 - acc: 0.400 - ETA: 4s - loss: 1.5874 - acc: 0.390 - ETA: 4s - loss: 1.5876 - acc: 0.371 - ETA: 4s - loss: 1.5337 - acc: 0.438 - ETA: 4s - loss: 1.5779 - acc: 0.431 - ETA: 4s - loss: 1.5944 - acc: 0.426 - ETA: 4s - loss: 1.5869 - acc: 0.420 - ETA: 4s - loss: 1.5801 - acc: 0.420 - ETA: 4s - loss: 1.5815 - acc: 0.421 - ETA: 4s - loss: 1.5474 - acc: 0.440 - ETA: 4s - loss: 1.5641 - acc: 0.432 - ETA: 4s - loss: 1.5497 - acc: 0.440 - ETA: 3s - loss: 1.5410 - acc: 0.448 - ETA: 3s - loss: 1.5205 - acc: 0.456 - ETA: 3s - loss: 1.5263 - acc: 0.455 - ETA: 3s - loss: 1.5413 - acc: 0.453 - ETA: 3s - loss: 1.5513 - acc: 0.448 - ETA: 3s - loss: 1.5407 - acc: 0.454 - ETA: 3s - loss: 1.5412 - acc: 0.455 - ETA: 3s - loss: 1.5436 - acc: 0.453 - ETA: 3s - loss: 1.5464 - acc: 0.452 - ETA: 3s - loss: 1.5455 - acc: 0.455 - ETA: 3s - loss: 1.5404 - acc: 0.459 - ETA: 3s - loss: 1.5346 - acc: 0.463 - ETA: 3s - loss: 1.5316 - acc: 0.464 - ETA: 3s - loss: 1.5292 - acc: 0.463 - ETA: 3s - loss: 1.5235 - acc: 0.465 - ETA: 3s - loss: 1.5163 - acc: 0.468 - ETA: 3s - loss: 1.5143 - acc: 0.468 - ETA: 3s - loss: 1.5181 - acc: 0.465 - ETA: 2s - loss: 1.5249 - acc: 0.462 - ETA: 2s - loss: 1.5247 - acc: 0.463 - ETA: 2s - loss: 1.5233 - acc: 0.464 - ETA: 2s - loss: 1.5250 - acc: 0.464 - ETA: 2s - loss: 1.5271 - acc: 0.462 - ETA: 2s - loss: 1.5315 - acc: 0.461 - ETA: 2s - loss: 1.5330 - acc: 0.463 - ETA: 2s - loss: 1.5299 - acc: 0.465 - ETA: 2s - loss: 1.5324 - acc: 0.466 - ETA: 2s - loss: 1.5366 - acc: 0.464 - ETA: 2s - loss: 1.5305 - acc: 0.468 - ETA: 2s - loss: 1.5324 - acc: 0.465 - ETA: 2s - loss: 1.5363 - acc: 0.462 - ETA: 2s - loss: 1.5359 - acc: 0.463 - ETA: 2s - loss: 1.5359 - acc: 0.462 - ETA: 1s - loss: 1.5344 - acc: 0.462 - ETA: 1s - loss: 1.5348 - acc: 0.461 - ETA: 1s - loss: 1.5336 - acc: 0.462 - ETA: 1s - loss: 1.5325 - acc: 0.462 - ETA: 1s - loss: 1.5323 - acc: 0.462 - ETA: 1s - loss: 1.5303 - acc: 0.462 - ETA: 1s - loss: 1.5335 - acc: 0.461 - ETA: 1s - loss: 1.5320 - acc: 0.461 - ETA: 1s - loss: 1.5295 - acc: 0.462 - ETA: 1s - loss: 1.5251 - acc: 0.465 - ETA: 1s - loss: 1.5251 - acc: 0.464 - ETA: 1s - loss: 1.5210 - acc: 0.464 - ETA: 1s - loss: 1.5252 - acc: 0.463 - ETA: 1s - loss: 1.5219 - acc: 0.465 - ETA: 1s - loss: 1.5213 - acc: 0.466 - ETA: 0s - loss: 1.5222 - acc: 0.466 - ETA: 0s - loss: 1.5222 - acc: 0.465 - ETA: 0s - loss: 1.5236 - acc: 0.465 - ETA: 0s - loss: 1.5221 - acc: 0.466 - ETA: 0s - loss: 1.5253 - acc: 0.465 - ETA: 0s - loss: 1.5266 - acc: 0.464 - ETA: 0s - loss: 1.5256 - acc: 0.465 - ETA: 0s - loss: 1.5258 - acc: 0.465 - ETA: 0s - loss: 1.5292 - acc: 0.464 - ETA: 0s - loss: 1.5275 - acc: 0.465 - ETA: 0s - loss: 1.5289 - acc: 0.465 - ETA: 0s - loss: 1.5255 - acc: 0.467 - ETA: 0s - loss: 1.5267 - acc: 0.466 - ETA: 0s - loss: 1.5294 - acc: 0.465 - ETA: 0s - loss: 1.5298 - acc: 0.465 - 6s 2ms/step - loss: 1.5320 - acc: 0.4651 - val_loss: 1.5298 - val_acc: 0.4749\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.54906 to 1.52976, saving model to saved_models/weights.best.benchmark.hdf5\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3021/3021 [==============================] - ETA: 4s - loss: 1.2671 - acc: 0.650 - ETA: 4s - loss: 1.3895 - acc: 0.533 - ETA: 4s - loss: 1.4639 - acc: 0.490 - ETA: 4s - loss: 1.4637 - acc: 0.492 - ETA: 4s - loss: 1.5018 - acc: 0.472 - ETA: 4s - loss: 1.4581 - acc: 0.500 - ETA: 4s - loss: 1.4396 - acc: 0.507 - ETA: 4s - loss: 1.4870 - acc: 0.486 - ETA: 4s - loss: 1.4906 - acc: 0.488 - ETA: 4s - loss: 1.4989 - acc: 0.478 - ETA: 4s - loss: 1.4911 - acc: 0.483 - ETA: 4s - loss: 1.4905 - acc: 0.476 - ETA: 4s - loss: 1.4935 - acc: 0.480 - ETA: 3s - loss: 1.4948 - acc: 0.475 - ETA: 3s - loss: 1.4853 - acc: 0.479 - ETA: 3s - loss: 1.4750 - acc: 0.480 - ETA: 3s - loss: 1.4655 - acc: 0.486 - ETA: 3s - loss: 1.4878 - acc: 0.478 - ETA: 3s - loss: 1.4834 - acc: 0.477 - ETA: 3s - loss: 1.4929 - acc: 0.478 - ETA: 3s - loss: 1.4955 - acc: 0.478 - ETA: 3s - loss: 1.4935 - acc: 0.479 - ETA: 3s - loss: 1.4960 - acc: 0.480 - ETA: 3s - loss: 1.4950 - acc: 0.481 - ETA: 3s - loss: 1.4906 - acc: 0.482 - ETA: 3s - loss: 1.4972 - acc: 0.480 - ETA: 3s - loss: 1.4965 - acc: 0.479 - ETA: 3s - loss: 1.4844 - acc: 0.485 - ETA: 3s - loss: 1.4874 - acc: 0.485 - ETA: 2s - loss: 1.4941 - acc: 0.481 - ETA: 2s - loss: 1.4911 - acc: 0.483 - ETA: 2s - loss: 1.4951 - acc: 0.480 - ETA: 2s - loss: 1.4896 - acc: 0.482 - ETA: 2s - loss: 1.4934 - acc: 0.479 - ETA: 2s - loss: 1.4997 - acc: 0.476 - ETA: 2s - loss: 1.5044 - acc: 0.472 - ETA: 2s - loss: 1.5085 - acc: 0.469 - ETA: 2s - loss: 1.5177 - acc: 0.464 - ETA: 2s - loss: 1.5170 - acc: 0.466 - ETA: 2s - loss: 1.5170 - acc: 0.466 - ETA: 2s - loss: 1.5159 - acc: 0.466 - ETA: 2s - loss: 1.5211 - acc: 0.466 - ETA: 2s - loss: 1.5217 - acc: 0.464 - ETA: 2s - loss: 1.5143 - acc: 0.467 - ETA: 2s - loss: 1.5154 - acc: 0.465 - ETA: 1s - loss: 1.5207 - acc: 0.462 - ETA: 1s - loss: 1.5230 - acc: 0.463 - ETA: 1s - loss: 1.5264 - acc: 0.462 - ETA: 1s - loss: 1.5238 - acc: 0.463 - ETA: 1s - loss: 1.5246 - acc: 0.464 - ETA: 1s - loss: 1.5248 - acc: 0.465 - ETA: 1s - loss: 1.5256 - acc: 0.465 - ETA: 1s - loss: 1.5274 - acc: 0.463 - ETA: 1s - loss: 1.5276 - acc: 0.463 - ETA: 1s - loss: 1.5305 - acc: 0.463 - ETA: 1s - loss: 1.5312 - acc: 0.463 - ETA: 1s - loss: 1.5306 - acc: 0.462 - ETA: 1s - loss: 1.5334 - acc: 0.460 - ETA: 1s - loss: 1.5332 - acc: 0.457 - ETA: 1s - loss: 1.5308 - acc: 0.458 - ETA: 1s - loss: 1.5302 - acc: 0.459 - ETA: 0s - loss: 1.5299 - acc: 0.459 - ETA: 0s - loss: 1.5260 - acc: 0.461 - ETA: 0s - loss: 1.5238 - acc: 0.461 - ETA: 0s - loss: 1.5174 - acc: 0.465 - ETA: 0s - loss: 1.5164 - acc: 0.465 - ETA: 0s - loss: 1.5215 - acc: 0.463 - ETA: 0s - loss: 1.5202 - acc: 0.463 - ETA: 0s - loss: 1.5194 - acc: 0.462 - ETA: 0s - loss: 1.5215 - acc: 0.462 - ETA: 0s - loss: 1.5221 - acc: 0.460 - ETA: 0s - loss: 1.5216 - acc: 0.461 - ETA: 0s - loss: 1.5201 - acc: 0.461 - ETA: 0s - loss: 1.5214 - acc: 0.460 - ETA: 0s - loss: 1.5218 - acc: 0.459 - ETA: 0s - loss: 1.5231 - acc: 0.459 - ETA: 0s - loss: 1.5221 - acc: 0.460 - 5s 2ms/step - loss: 1.5217 - acc: 0.4608 - val_loss: 1.5163 - val_acc: 0.4828\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.52976 to 1.51633, saving model to saved_models/weights.best.benchmark.hdf5\n",
      "Epoch 10/10\n",
      "3021/3021 [==============================] - ETA: 4s - loss: 1.4880 - acc: 0.550 - ETA: 4s - loss: 1.6005 - acc: 0.400 - ETA: 4s - loss: 1.6045 - acc: 0.390 - ETA: 4s - loss: 1.5511 - acc: 0.428 - ETA: 5s - loss: 1.5231 - acc: 0.450 - ETA: 4s - loss: 1.5463 - acc: 0.440 - ETA: 4s - loss: 1.5206 - acc: 0.441 - ETA: 4s - loss: 1.4896 - acc: 0.457 - ETA: 4s - loss: 1.4762 - acc: 0.462 - ETA: 4s - loss: 1.4393 - acc: 0.486 - ETA: 4s - loss: 1.4373 - acc: 0.482 - ETA: 4s - loss: 1.4626 - acc: 0.475 - ETA: 4s - loss: 1.4664 - acc: 0.470 - ETA: 4s - loss: 1.4932 - acc: 0.459 - ETA: 4s - loss: 1.4908 - acc: 0.455 - ETA: 4s - loss: 1.4753 - acc: 0.466 - ETA: 3s - loss: 1.4589 - acc: 0.468 - ETA: 3s - loss: 1.4736 - acc: 0.466 - ETA: 3s - loss: 1.4733 - acc: 0.466 - ETA: 3s - loss: 1.4772 - acc: 0.466 - ETA: 3s - loss: 1.4782 - acc: 0.467 - ETA: 3s - loss: 1.4764 - acc: 0.472 - ETA: 3s - loss: 1.4860 - acc: 0.468 - ETA: 3s - loss: 1.4848 - acc: 0.467 - ETA: 3s - loss: 1.4906 - acc: 0.468 - ETA: 3s - loss: 1.4861 - acc: 0.472 - ETA: 3s - loss: 1.4807 - acc: 0.474 - ETA: 3s - loss: 1.4790 - acc: 0.474 - ETA: 3s - loss: 1.4813 - acc: 0.474 - ETA: 3s - loss: 1.4715 - acc: 0.477 - ETA: 3s - loss: 1.4784 - acc: 0.473 - ETA: 3s - loss: 1.4910 - acc: 0.469 - ETA: 2s - loss: 1.4829 - acc: 0.473 - ETA: 2s - loss: 1.4861 - acc: 0.471 - ETA: 2s - loss: 1.4927 - acc: 0.466 - ETA: 2s - loss: 1.4980 - acc: 0.463 - ETA: 2s - loss: 1.4925 - acc: 0.466 - ETA: 2s - loss: 1.4930 - acc: 0.466 - ETA: 2s - loss: 1.4924 - acc: 0.467 - ETA: 2s - loss: 1.5035 - acc: 0.463 - ETA: 2s - loss: 1.5025 - acc: 0.463 - ETA: 2s - loss: 1.5027 - acc: 0.464 - ETA: 2s - loss: 1.5040 - acc: 0.465 - ETA: 2s - loss: 1.5054 - acc: 0.466 - ETA: 2s - loss: 1.5066 - acc: 0.466 - ETA: 2s - loss: 1.5046 - acc: 0.464 - ETA: 2s - loss: 1.5075 - acc: 0.464 - ETA: 1s - loss: 1.5053 - acc: 0.466 - ETA: 1s - loss: 1.5055 - acc: 0.467 - ETA: 1s - loss: 1.5017 - acc: 0.469 - ETA: 1s - loss: 1.5040 - acc: 0.469 - ETA: 1s - loss: 1.5049 - acc: 0.469 - ETA: 1s - loss: 1.5012 - acc: 0.470 - ETA: 1s - loss: 1.5027 - acc: 0.471 - ETA: 1s - loss: 1.5026 - acc: 0.470 - ETA: 1s - loss: 1.5011 - acc: 0.470 - ETA: 1s - loss: 1.5040 - acc: 0.468 - ETA: 1s - loss: 1.5041 - acc: 0.469 - ETA: 1s - loss: 1.5096 - acc: 0.466 - ETA: 1s - loss: 1.5089 - acc: 0.467 - ETA: 1s - loss: 1.5066 - acc: 0.469 - ETA: 1s - loss: 1.5051 - acc: 0.469 - ETA: 0s - loss: 1.5075 - acc: 0.468 - ETA: 0s - loss: 1.5050 - acc: 0.469 - ETA: 0s - loss: 1.5069 - acc: 0.468 - ETA: 0s - loss: 1.5062 - acc: 0.469 - ETA: 0s - loss: 1.5061 - acc: 0.469 - ETA: 0s - loss: 1.5038 - acc: 0.471 - ETA: 0s - loss: 1.5071 - acc: 0.468 - ETA: 0s - loss: 1.5077 - acc: 0.468 - ETA: 0s - loss: 1.5078 - acc: 0.468 - ETA: 0s - loss: 1.5091 - acc: 0.467 - ETA: 0s - loss: 1.5112 - acc: 0.464 - ETA: 0s - loss: 1.5120 - acc: 0.464 - ETA: 0s - loss: 1.5145 - acc: 0.465 - ETA: 0s - loss: 1.5131 - acc: 0.465 - ETA: 0s - loss: 1.5146 - acc: 0.465 - 6s 2ms/step - loss: 1.5150 - acc: 0.4651 - val_loss: 1.5392 - val_acc: 0.4815\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.51633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x157ffa88198>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#checkpointer saves the best weights.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.benchmark.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "benchmark.fit(train_tensors, train_targets, batch_size=20, epochs=epochs, callbacks=[checkpointer], validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the  weights of Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.load_weights('saved_models/weights.best.benchmark.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model_prediction = [benchmark.predict(np.expand_dims(img_tensor, axis=0)) for img_tensor in test_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42553875 0.05191546 0.03911333 0.01771741 0.06787238 0.06665134\n",
      "  0.0635898  0.26760146]]\n",
      "        ALB       BET       DOL       LAG       NoF     OTHER     SHARK  \\\n",
      "0  0.425539  0.051915  0.039113  0.017717  0.067872  0.066651  0.063590   \n",
      "1  0.479572  0.043038  0.030075  0.020164  0.235390  0.055788  0.014474   \n",
      "2  0.432765  0.043701  0.026840  0.024427  0.299017  0.066578  0.013751   \n",
      "3  0.506477  0.047196  0.027785  0.024373  0.201054  0.063204  0.020254   \n",
      "4  0.464244  0.044821  0.029505  0.022817  0.252187  0.061723  0.015182   \n",
      "\n",
      "        YFT  \n",
      "0  0.267601  \n",
      "1  0.121501  \n",
      "2  0.092921  \n",
      "3  0.109657  \n",
      "4  0.109522  \n"
     ]
    }
   ],
   "source": [
    "#visaulizing the array\n",
    "print(benchmark_model_prediction[:][0])\n",
    "\n",
    "#swapping the axes of the benchmark_model_prediction for easy handling\n",
    "benchmark_model_prediction = np.swapaxes(benchmark_model_prediction,0,1)\n",
    "\n",
    "#creating a pandas dataframe for with benchmark model's prediction\n",
    "df_pred_model1 = pd.DataFrame(benchmark_model_prediction[0][:], columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])\n",
    "\n",
    "#first five rows of df_pred_model1 dataframe\n",
    "print(df_pred_model1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_files[0]\n",
    "\n",
    "#extracting relevant name of the image from the full-path of image\n",
    "image_names = [test_files[i][15:] for i in range(len(test_files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting the filename of the image to match the submission guidelines\n",
    "for i in range(13153):\n",
    "    if image_names[i][5]=='_':\n",
    "        image_names[i] = \"test_stg2/\" + image_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_stg2/image_04056.jpg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_names[1323]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image       ALB       BET       DOL       LAG  \\\n",
      "0  test_stg2/image_10973.jpg  0.425539  0.051915  0.039113  0.017717   \n",
      "1  test_stg2/image_00175.jpg  0.479572  0.043038  0.030075  0.020164   \n",
      "2  test_stg2/image_09645.jpg  0.432765  0.043701  0.026840  0.024427   \n",
      "3              img_02920.jpg  0.506477  0.047196  0.027785  0.024373   \n",
      "4  test_stg2/image_09349.jpg  0.464244  0.044821  0.029505  0.022817   \n",
      "\n",
      "        NoF     OTHER     SHARK       YFT  \n",
      "0  0.067872  0.066651  0.063590  0.267601  \n",
      "1  0.235390  0.055788  0.014474  0.121501  \n",
      "2  0.299017  0.066578  0.013751  0.092921  \n",
      "3  0.201054  0.063204  0.020254  0.109657  \n",
      "4  0.252187  0.061723  0.015182  0.109522  \n"
     ]
    }
   ],
   "source": [
    "#adding image names to our dataframe\n",
    "df_pred_model1['image'] = pd.DataFrame(image_names)\n",
    "\n",
    "#reindexing the dataframe\n",
    "df_pred_model1 = df_pred_model1.reindex_axis(['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'], axis=1)\n",
    "\n",
    "#printing the first five rows of dataframe\n",
    "print(df_pred_model1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>test_stg2/image_05875.jpg</td>\n",
       "      <td>0.515315</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>0.016914</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.047733</td>\n",
       "      <td>0.015448</td>\n",
       "      <td>0.151607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13144</th>\n",
       "      <td>test_stg2/image_04374.jpg</td>\n",
       "      <td>0.540486</td>\n",
       "      <td>0.040701</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.013437</td>\n",
       "      <td>0.118196</td>\n",
       "      <td>0.038110</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>0.194842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13145</th>\n",
       "      <td>test_stg2/image_07892.jpg</td>\n",
       "      <td>0.452034</td>\n",
       "      <td>0.052957</td>\n",
       "      <td>0.033044</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.106285</td>\n",
       "      <td>0.081277</td>\n",
       "      <td>0.050915</td>\n",
       "      <td>0.202207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13146</th>\n",
       "      <td>test_stg2/image_09226.jpg</td>\n",
       "      <td>0.450020</td>\n",
       "      <td>0.052148</td>\n",
       "      <td>0.036669</td>\n",
       "      <td>0.020324</td>\n",
       "      <td>0.128799</td>\n",
       "      <td>0.070184</td>\n",
       "      <td>0.037010</td>\n",
       "      <td>0.204846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13147</th>\n",
       "      <td>test_stg2/image_04860.jpg</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>0.024683</td>\n",
       "      <td>0.448714</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.093120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13148</th>\n",
       "      <td>img_07578.jpg</td>\n",
       "      <td>0.501905</td>\n",
       "      <td>0.041839</td>\n",
       "      <td>0.035838</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.145416</td>\n",
       "      <td>0.042610</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.200671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13149</th>\n",
       "      <td>test_stg2/image_03265.jpg</td>\n",
       "      <td>0.556458</td>\n",
       "      <td>0.035502</td>\n",
       "      <td>0.025563</td>\n",
       "      <td>0.013570</td>\n",
       "      <td>0.167819</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>0.145767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13150</th>\n",
       "      <td>test_stg2/image_09846.jpg</td>\n",
       "      <td>0.437534</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.027692</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.052302</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.137225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13151</th>\n",
       "      <td>test_stg2/image_10800.jpg</td>\n",
       "      <td>0.509217</td>\n",
       "      <td>0.047430</td>\n",
       "      <td>0.028931</td>\n",
       "      <td>0.018075</td>\n",
       "      <td>0.084743</td>\n",
       "      <td>0.067471</td>\n",
       "      <td>0.049382</td>\n",
       "      <td>0.194752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13152</th>\n",
       "      <td>test_stg2/image_02733.jpg</td>\n",
       "      <td>0.484187</td>\n",
       "      <td>0.037991</td>\n",
       "      <td>0.033073</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0.196655</td>\n",
       "      <td>0.043481</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.179210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image       ALB       BET       DOL       LAG  \\\n",
       "13143  test_stg2/image_05875.jpg  0.515315  0.041940  0.032962  0.016914   \n",
       "13144  test_stg2/image_04374.jpg  0.540486  0.040701  0.034733  0.013437   \n",
       "13145  test_stg2/image_07892.jpg  0.452034  0.052957  0.033044  0.021281   \n",
       "13146  test_stg2/image_09226.jpg  0.450020  0.052148  0.036669  0.020324   \n",
       "13147  test_stg2/image_04860.jpg  0.276634  0.042274  0.030476  0.024683   \n",
       "13148              img_07578.jpg  0.501905  0.041839  0.035838  0.014108   \n",
       "13149  test_stg2/image_03265.jpg  0.556458  0.035502  0.025563  0.013570   \n",
       "13150  test_stg2/image_09846.jpg  0.437534  0.036371  0.027692  0.015496   \n",
       "13151  test_stg2/image_10800.jpg  0.509217  0.047430  0.028931  0.018075   \n",
       "13152  test_stg2/image_02733.jpg  0.484187  0.037991  0.033073  0.013554   \n",
       "\n",
       "            NoF     OTHER     SHARK       YFT  \n",
       "13143  0.178082  0.047733  0.015448  0.151607  \n",
       "13144  0.118196  0.038110  0.019496  0.194842  \n",
       "13145  0.106285  0.081277  0.050915  0.202207  \n",
       "13146  0.128799  0.070184  0.037010  0.204846  \n",
       "13147  0.448714  0.074510  0.009589  0.093120  \n",
       "13148  0.145416  0.042610  0.017613  0.200671  \n",
       "13149  0.167819  0.042091  0.013229  0.145767  \n",
       "13150  0.283582  0.052302  0.009798  0.137225  \n",
       "13151  0.084743  0.067471  0.049382  0.194752  \n",
       "13152  0.196655  0.043481  0.011850  0.179210  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_model1.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  .csv file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_model1.to_csv('submission1-benchmark.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
