{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the dataset\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    fish_files = np.array(data['filenames'])\n",
    "    fish_target = np_utils.to_categorical(np.array(data['target']), 8)\n",
    "    return fish_files,fish_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3777 images in training dataset\n",
      "There are 13153 images in the training set\n"
     ]
    }
   ],
   "source": [
    "#let's load the training-data\n",
    "train_files, train_targets = load_dataset('data/train')\n",
    "\n",
    "#let's load the teting-data\n",
    "test_files, _ = load_dataset('data/test')\n",
    "\n",
    "#print the number of samples in test and trainin sets\n",
    "print (\"There are %d images in training dataset\"%len(train_files))\n",
    "print (\"There are %d images in the training set\"%len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "#converting image to tensor\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image\n",
    "    img = image.load_img(img_path, target_size=(224,224))\n",
    "    #convering the image to 3-D tensor with shape (224,224,3)\n",
    "    x = image.img_to_array(img)\n",
    "    #convert 3D tensor to 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13153/13153 [02:07<00:00, 103.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "\n",
    "#preprocessing the data\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3777/3777 [01:37<00:00, 38.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3777, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#shape of the tensor\n",
    "print(np.shape(train_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-6, Refining Model-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 223, 223, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 110, 110, 64)      8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 55, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 55, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 54, 54, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27, 27, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 42,600\n",
      "Trainable params: 42,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model6 = Sequential()\n",
    "\n",
    "# model Convolution layer\n",
    "model6.add(Conv2D(filters=32,kernel_size=2,strides=1,activation='relu',input_shape=(224,224,3)))\n",
    "# Max Pooling layer to reduce the dimensionality\n",
    "model6.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "#Dropout layer, for turning off each node with the probability of 0.2 \n",
    "model6.add(Dropout(0.2))\n",
    "model6.add(Conv2D(filters=64, kernel_size=2,strides=1,activation='relu'))\n",
    "model6.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "#Dropout layer, for turning off each node with the probability of 0.2\n",
    "model6.add(Dropout(0.2))\n",
    "model6.add(Conv2D(filters=128,kernel_size=2,strides=1,activation='relu'))\n",
    "model6.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "#Dropout layer, for turning off each node with the probability of 0.2\n",
    "model6.add(Dropout(0.2))\n",
    "model6.add(GlobalAveragePooling2D())\n",
    "#A fully connected dense layer with 8 nodes (no of classes of fish)\n",
    "model6.add(Dense(8,activation='softmax'))\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2643 samples, validate on 1134 samples\n",
      "Epoch 1/10\n",
      "2643/2643 [==============================] - ETA: 13:25 - loss: 2.0899 - acc: 0.10 - ETA: 6:42 - loss: 1.9817 - acc: 0.1500 - ETA: 4:28 - loss: 1.9097 - acc: 0.266 - ETA: 2:40 - loss: 1.8479 - acc: 0.310 - ETA: 1:54 - loss: 1.6888 - acc: 0.385 - ETA: 1:29 - loss: 1.7425 - acc: 0.388 - ETA: 1:12 - loss: 1.7238 - acc: 0.400 - ETA: 1:01 - loss: 1.7348 - acc: 0.392 - ETA: 52s - loss: 1.7289 - acc: 0.400 - ETA: 46s - loss: 1.7198 - acc: 0.40 - ETA: 41s - loss: 1.7166 - acc: 0.40 - ETA: 37s - loss: 1.7087 - acc: 0.40 - ETA: 33s - loss: 1.7146 - acc: 0.40 - ETA: 32s - loss: 1.7096 - acc: 0.40 - ETA: 31s - loss: 1.7058 - acc: 0.40 - ETA: 28s - loss: 1.6865 - acc: 0.42 - ETA: 26s - loss: 1.6672 - acc: 0.43 - ETA: 24s - loss: 1.6790 - acc: 0.42 - ETA: 22s - loss: 1.6821 - acc: 0.42 - ETA: 21s - loss: 1.6751 - acc: 0.43 - ETA: 20s - loss: 1.6776 - acc: 0.43 - ETA: 18s - loss: 1.6781 - acc: 0.43 - ETA: 17s - loss: 1.6599 - acc: 0.44 - ETA: 16s - loss: 1.6605 - acc: 0.44 - ETA: 15s - loss: 1.6529 - acc: 0.44 - ETA: 14s - loss: 1.6540 - acc: 0.44 - ETA: 14s - loss: 1.6520 - acc: 0.45 - ETA: 13s - loss: 1.6560 - acc: 0.45 - ETA: 13s - loss: 1.6574 - acc: 0.44 - ETA: 12s - loss: 1.6631 - acc: 0.44 - ETA: 11s - loss: 1.6693 - acc: 0.43 - ETA: 11s - loss: 1.6679 - acc: 0.43 - ETA: 10s - loss: 1.6623 - acc: 0.44 - ETA: 10s - loss: 1.6671 - acc: 0.43 - ETA: 9s - loss: 1.6685 - acc: 0.4406 - ETA: 9s - loss: 1.6661 - acc: 0.441 - ETA: 8s - loss: 1.6641 - acc: 0.441 - ETA: 8s - loss: 1.6602 - acc: 0.443 - ETA: 7s - loss: 1.6507 - acc: 0.446 - ETA: 7s - loss: 1.6507 - acc: 0.447 - ETA: 7s - loss: 1.6470 - acc: 0.450 - ETA: 6s - loss: 1.6393 - acc: 0.453 - ETA: 6s - loss: 1.6337 - acc: 0.454 - ETA: 6s - loss: 1.6275 - acc: 0.456 - ETA: 5s - loss: 1.6278 - acc: 0.456 - ETA: 5s - loss: 1.6253 - acc: 0.455 - ETA: 5s - loss: 1.6181 - acc: 0.458 - ETA: 4s - loss: 1.6178 - acc: 0.457 - ETA: 4s - loss: 1.6143 - acc: 0.459 - ETA: 4s - loss: 1.6142 - acc: 0.460 - ETA: 3s - loss: 1.6220 - acc: 0.457 - ETA: 3s - loss: 1.6257 - acc: 0.455 - ETA: 3s - loss: 1.6226 - acc: 0.457 - ETA: 3s - loss: 1.6201 - acc: 0.457 - ETA: 3s - loss: 1.6200 - acc: 0.457 - ETA: 2s - loss: 1.6231 - acc: 0.456 - ETA: 2s - loss: 1.6215 - acc: 0.457 - ETA: 2s - loss: 1.6235 - acc: 0.456 - ETA: 2s - loss: 1.6240 - acc: 0.455 - ETA: 1s - loss: 1.6213 - acc: 0.455 - ETA: 1s - loss: 1.6153 - acc: 0.458 - ETA: 1s - loss: 1.6169 - acc: 0.456 - ETA: 1s - loss: 1.6163 - acc: 0.455 - ETA: 1s - loss: 1.6157 - acc: 0.456 - ETA: 0s - loss: 1.6131 - acc: 0.458 - ETA: 0s - loss: 1.6130 - acc: 0.456 - ETA: 0s - loss: 1.6131 - acc: 0.457 - ETA: 0s - loss: 1.6121 - acc: 0.457 - ETA: 0s - loss: 1.6126 - acc: 0.456 - ETA: 0s - loss: 1.6167 - acc: 0.454 - 13s 5ms/step - loss: 1.6176 - acc: 0.4537 - val_loss: 1.7716 - val_acc: 0.4347\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77164, saving model to saved_models/weights.best.model6.hdf5\n",
      "Epoch 2/10\n",
      "2643/2643 [==============================] - ETA: 5s - loss: 1.6417 - acc: 0.500 - ETA: 5s - loss: 1.6166 - acc: 0.433 - ETA: 5s - loss: 1.5926 - acc: 0.440 - ETA: 5s - loss: 1.5567 - acc: 0.458 - ETA: 5s - loss: 1.5184 - acc: 0.468 - ETA: 5s - loss: 1.4908 - acc: 0.485 - ETA: 5s - loss: 1.4917 - acc: 0.481 - ETA: 5s - loss: 1.4801 - acc: 0.488 - ETA: 5s - loss: 1.4684 - acc: 0.493 - ETA: 5s - loss: 1.4625 - acc: 0.494 - ETA: 5s - loss: 1.4905 - acc: 0.481 - ETA: 5s - loss: 1.5253 - acc: 0.461 - ETA: 5s - loss: 1.5338 - acc: 0.456 - ETA: 5s - loss: 1.5245 - acc: 0.462 - ETA: 4s - loss: 1.5263 - acc: 0.466 - ETA: 4s - loss: 1.5278 - acc: 0.469 - ETA: 4s - loss: 1.5348 - acc: 0.465 - ETA: 4s - loss: 1.5287 - acc: 0.470 - ETA: 4s - loss: 1.5316 - acc: 0.470 - ETA: 4s - loss: 1.5267 - acc: 0.469 - ETA: 4s - loss: 1.5351 - acc: 0.467 - ETA: 4s - loss: 1.5350 - acc: 0.467 - ETA: 4s - loss: 1.5406 - acc: 0.469 - ETA: 4s - loss: 1.5350 - acc: 0.471 - ETA: 3s - loss: 1.5357 - acc: 0.470 - ETA: 3s - loss: 1.5398 - acc: 0.469 - ETA: 3s - loss: 1.5306 - acc: 0.472 - ETA: 3s - loss: 1.5378 - acc: 0.469 - ETA: 3s - loss: 1.5365 - acc: 0.469 - ETA: 3s - loss: 1.5379 - acc: 0.470 - ETA: 3s - loss: 1.5368 - acc: 0.472 - ETA: 3s - loss: 1.5446 - acc: 0.469 - ETA: 3s - loss: 1.5420 - acc: 0.473 - ETA: 3s - loss: 1.5396 - acc: 0.472 - ETA: 3s - loss: 1.5373 - acc: 0.474 - ETA: 3s - loss: 1.5350 - acc: 0.476 - ETA: 2s - loss: 1.5333 - acc: 0.476 - ETA: 2s - loss: 1.5334 - acc: 0.477 - ETA: 2s - loss: 1.5222 - acc: 0.482 - ETA: 2s - loss: 1.5225 - acc: 0.483 - ETA: 2s - loss: 1.5258 - acc: 0.481 - ETA: 2s - loss: 1.5250 - acc: 0.482 - ETA: 2s - loss: 1.5295 - acc: 0.481 - ETA: 2s - loss: 1.5259 - acc: 0.484 - ETA: 2s - loss: 1.5245 - acc: 0.484 - ETA: 2s - loss: 1.5269 - acc: 0.483 - ETA: 2s - loss: 1.5300 - acc: 0.478 - ETA: 2s - loss: 1.5320 - acc: 0.477 - ETA: 1s - loss: 1.5402 - acc: 0.472 - ETA: 1s - loss: 1.5413 - acc: 0.472 - ETA: 1s - loss: 1.5404 - acc: 0.473 - ETA: 1s - loss: 1.5455 - acc: 0.472 - ETA: 1s - loss: 1.5448 - acc: 0.470 - ETA: 1s - loss: 1.5423 - acc: 0.472 - ETA: 1s - loss: 1.5427 - acc: 0.472 - ETA: 1s - loss: 1.5409 - acc: 0.470 - ETA: 1s - loss: 1.5389 - acc: 0.471 - ETA: 1s - loss: 1.5446 - acc: 0.469 - ETA: 1s - loss: 1.5452 - acc: 0.467 - ETA: 0s - loss: 1.5433 - acc: 0.468 - ETA: 0s - loss: 1.5429 - acc: 0.469 - ETA: 0s - loss: 1.5485 - acc: 0.466 - ETA: 0s - loss: 1.5482 - acc: 0.465 - ETA: 0s - loss: 1.5471 - acc: 0.465 - ETA: 0s - loss: 1.5490 - acc: 0.466 - ETA: 0s - loss: 1.5508 - acc: 0.465 - ETA: 0s - loss: 1.5510 - acc: 0.464 - ETA: 0s - loss: 1.5474 - acc: 0.467 - ETA: 0s - loss: 1.5459 - acc: 0.468 - ETA: 0s - loss: 1.5470 - acc: 0.467 - ETA: 0s - loss: 1.5492 - acc: 0.466 - 7s 3ms/step - loss: 1.5496 - acc: 0.4673 - val_loss: 1.5388 - val_acc: 0.4480\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.77164 to 1.53881, saving model to saved_models/weights.best.model6.hdf5\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2643/2643 [==============================] - ETA: 6s - loss: 1.6208 - acc: 0.400 - ETA: 6s - loss: 1.7493 - acc: 0.383 - ETA: 6s - loss: 1.6676 - acc: 0.420 - ETA: 5s - loss: 1.6342 - acc: 0.435 - ETA: 5s - loss: 1.6283 - acc: 0.427 - ETA: 5s - loss: 1.6160 - acc: 0.431 - ETA: 5s - loss: 1.5756 - acc: 0.442 - ETA: 5s - loss: 1.5987 - acc: 0.430 - ETA: 5s - loss: 1.5915 - acc: 0.426 - ETA: 5s - loss: 1.6048 - acc: 0.423 - ETA: 5s - loss: 1.5973 - acc: 0.426 - ETA: 5s - loss: 1.6057 - acc: 0.421 - ETA: 4s - loss: 1.5879 - acc: 0.430 - ETA: 4s - loss: 1.5981 - acc: 0.424 - ETA: 4s - loss: 1.5781 - acc: 0.437 - ETA: 4s - loss: 1.5771 - acc: 0.438 - ETA: 4s - loss: 1.5697 - acc: 0.440 - ETA: 4s - loss: 1.5723 - acc: 0.437 - ETA: 4s - loss: 1.5699 - acc: 0.436 - ETA: 4s - loss: 1.5785 - acc: 0.435 - ETA: 4s - loss: 1.5556 - acc: 0.448 - ETA: 4s - loss: 1.5559 - acc: 0.450 - ETA: 3s - loss: 1.5544 - acc: 0.447 - ETA: 3s - loss: 1.5505 - acc: 0.445 - ETA: 3s - loss: 1.5507 - acc: 0.444 - ETA: 3s - loss: 1.5442 - acc: 0.449 - ETA: 3s - loss: 1.5358 - acc: 0.452 - ETA: 3s - loss: 1.5340 - acc: 0.455 - ETA: 3s - loss: 1.5369 - acc: 0.454 - ETA: 3s - loss: 1.5309 - acc: 0.457 - ETA: 3s - loss: 1.5304 - acc: 0.458 - ETA: 3s - loss: 1.5290 - acc: 0.459 - ETA: 3s - loss: 1.5220 - acc: 0.460 - ETA: 2s - loss: 1.5190 - acc: 0.460 - ETA: 2s - loss: 1.5171 - acc: 0.461 - ETA: 2s - loss: 1.5151 - acc: 0.462 - ETA: 2s - loss: 1.5156 - acc: 0.462 - ETA: 2s - loss: 1.5208 - acc: 0.460 - ETA: 2s - loss: 1.5189 - acc: 0.463 - ETA: 2s - loss: 1.5136 - acc: 0.465 - ETA: 2s - loss: 1.5205 - acc: 0.463 - ETA: 2s - loss: 1.5140 - acc: 0.466 - ETA: 2s - loss: 1.5133 - acc: 0.464 - ETA: 2s - loss: 1.5148 - acc: 0.462 - ETA: 1s - loss: 1.5158 - acc: 0.463 - ETA: 1s - loss: 1.5134 - acc: 0.464 - ETA: 1s - loss: 1.5118 - acc: 0.465 - ETA: 1s - loss: 1.5089 - acc: 0.466 - ETA: 1s - loss: 1.5066 - acc: 0.468 - ETA: 1s - loss: 1.5054 - acc: 0.467 - ETA: 1s - loss: 1.5025 - acc: 0.468 - ETA: 1s - loss: 1.5026 - acc: 0.468 - ETA: 1s - loss: 1.5028 - acc: 0.467 - ETA: 1s - loss: 1.5042 - acc: 0.467 - ETA: 1s - loss: 1.5018 - acc: 0.469 - ETA: 1s - loss: 1.5015 - acc: 0.470 - ETA: 0s - loss: 1.5029 - acc: 0.468 - ETA: 0s - loss: 1.5029 - acc: 0.468 - ETA: 0s - loss: 1.5027 - acc: 0.469 - ETA: 0s - loss: 1.4971 - acc: 0.472 - ETA: 0s - loss: 1.4951 - acc: 0.473 - ETA: 0s - loss: 1.4964 - acc: 0.472 - ETA: 0s - loss: 1.4983 - acc: 0.472 - ETA: 0s - loss: 1.5002 - acc: 0.472 - ETA: 0s - loss: 1.5014 - acc: 0.470 - ETA: 0s - loss: 1.5042 - acc: 0.468 - ETA: 0s - loss: 1.5046 - acc: 0.469 - ETA: 0s - loss: 1.5013 - acc: 0.469 - 7s 3ms/step - loss: 1.4996 - acc: 0.4707 - val_loss: 1.5048 - val_acc: 0.4594\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.53881 to 1.50482, saving model to saved_models/weights.best.model6.hdf5\n",
      "Epoch 4/10\n",
      "2643/2643 [==============================] - ETA: 6s - loss: 1.4722 - acc: 0.550 - ETA: 6s - loss: 1.4966 - acc: 0.516 - ETA: 5s - loss: 1.5585 - acc: 0.470 - ETA: 5s - loss: 1.5437 - acc: 0.450 - ETA: 5s - loss: 1.5309 - acc: 0.456 - ETA: 5s - loss: 1.4691 - acc: 0.480 - ETA: 5s - loss: 1.4200 - acc: 0.495 - ETA: 5s - loss: 1.3814 - acc: 0.510 - ETA: 5s - loss: 1.4105 - acc: 0.512 - ETA: 5s - loss: 1.3955 - acc: 0.527 - ETA: 5s - loss: 1.4120 - acc: 0.525 - ETA: 5s - loss: 1.3761 - acc: 0.536 - ETA: 4s - loss: 1.3700 - acc: 0.537 - ETA: 4s - loss: 1.3827 - acc: 0.530 - ETA: 4s - loss: 1.3976 - acc: 0.521 - ETA: 4s - loss: 1.4110 - acc: 0.516 - ETA: 4s - loss: 1.4143 - acc: 0.510 - ETA: 4s - loss: 1.4155 - acc: 0.508 - ETA: 4s - loss: 1.4133 - acc: 0.502 - ETA: 4s - loss: 1.4392 - acc: 0.496 - ETA: 4s - loss: 1.4328 - acc: 0.496 - ETA: 4s - loss: 1.4363 - acc: 0.492 - ETA: 4s - loss: 1.4408 - acc: 0.489 - ETA: 3s - loss: 1.4511 - acc: 0.485 - ETA: 3s - loss: 1.4616 - acc: 0.481 - ETA: 3s - loss: 1.4626 - acc: 0.479 - ETA: 3s - loss: 1.4657 - acc: 0.480 - ETA: 3s - loss: 1.4682 - acc: 0.481 - ETA: 3s - loss: 1.4736 - acc: 0.479 - ETA: 3s - loss: 1.4747 - acc: 0.478 - ETA: 3s - loss: 1.4742 - acc: 0.476 - ETA: 3s - loss: 1.4779 - acc: 0.474 - ETA: 3s - loss: 1.4721 - acc: 0.477 - ETA: 3s - loss: 1.4795 - acc: 0.474 - ETA: 2s - loss: 1.4850 - acc: 0.471 - ETA: 2s - loss: 1.4837 - acc: 0.471 - ETA: 2s - loss: 1.4804 - acc: 0.472 - ETA: 2s - loss: 1.4819 - acc: 0.471 - ETA: 2s - loss: 1.4828 - acc: 0.474 - ETA: 2s - loss: 1.4804 - acc: 0.475 - ETA: 2s - loss: 1.4757 - acc: 0.478 - ETA: 2s - loss: 1.4712 - acc: 0.482 - ETA: 2s - loss: 1.4709 - acc: 0.481 - ETA: 2s - loss: 1.4705 - acc: 0.481 - ETA: 2s - loss: 1.4771 - acc: 0.479 - ETA: 1s - loss: 1.4777 - acc: 0.478 - ETA: 1s - loss: 1.4771 - acc: 0.475 - ETA: 1s - loss: 1.4760 - acc: 0.476 - ETA: 1s - loss: 1.4765 - acc: 0.477 - ETA: 1s - loss: 1.4742 - acc: 0.479 - ETA: 1s - loss: 1.4732 - acc: 0.478 - ETA: 1s - loss: 1.4721 - acc: 0.480 - ETA: 1s - loss: 1.4719 - acc: 0.479 - ETA: 1s - loss: 1.4730 - acc: 0.478 - ETA: 1s - loss: 1.4676 - acc: 0.482 - ETA: 1s - loss: 1.4672 - acc: 0.482 - ETA: 1s - loss: 1.4643 - acc: 0.483 - ETA: 0s - loss: 1.4642 - acc: 0.483 - ETA: 0s - loss: 1.4603 - acc: 0.483 - ETA: 0s - loss: 1.4619 - acc: 0.483 - ETA: 0s - loss: 1.4607 - acc: 0.482 - ETA: 0s - loss: 1.4622 - acc: 0.483 - ETA: 0s - loss: 1.4622 - acc: 0.483 - ETA: 0s - loss: 1.4625 - acc: 0.482 - ETA: 0s - loss: 1.4601 - acc: 0.484 - ETA: 0s - loss: 1.4603 - acc: 0.483 - ETA: 0s - loss: 1.4636 - acc: 0.481 - ETA: 0s - loss: 1.4655 - acc: 0.480 - ETA: 0s - loss: 1.4644 - acc: 0.479 - 7s 3ms/step - loss: 1.4648 - acc: 0.4798 - val_loss: 1.5627 - val_acc: 0.3995\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.50482\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1988804cd68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#checkpointer saves the weight of the best model only\n",
    "checkpointer_6 = [EarlyStopping(monitor='val_loss',min_delta=0.01, patience=0, verbose=1), ModelCheckpoint(filepath='saved_models/weights.best.model6.hdf5',\n",
    "                                  verbose=1, save_best_only=True)]\n",
    "\n",
    "model6.fit(train_tensors, train_targets, batch_size=20, epochs=epochs, callbacks=checkpointer_6, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for Model-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the  weights of Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the weights of pretrained model\n",
    "model6.load_weights('saved_models/weights.best.model6.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "model6_prediction = [model6.predict(np.expand_dims(img_tensor, axis=0)) for img_tensor in test_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swapping the axes of the model6_prediction for easy handling\n",
    "model6_prediction = np.swapaxes(model6_prediction,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#creating a pandas dataframe for with benchmark model's prediction\n",
    "df_pred_model6 = pd.DataFrame(model6_prediction[0][:], columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_files[0]\n",
    "\n",
    "#extracting relevant name of the image from the full-path of image\n",
    "image_names = [test_files[i][15:] for i in range(len(test_files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting the filename of the image to match the submission guidelines\n",
    "for i in range(13153):\n",
    "    if image_names[i][5]=='_':\n",
    "        image_names[i] = \"test_stg2/\" + image_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#adding image names to our dataframe\n",
    "df_pred_model6['image'] = pd.DataFrame(image_names)\n",
    "\n",
    "#reindexing the dataframe\n",
    "df_pred_model6 = df_pred_model6.reindex_axis(['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  .csv file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_model6.to_csv('submission6.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Score - 1.51393 and Private Score - 1.70159"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
